{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c0a9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_catalog_data(n_products):\n",
    "\n",
    "    fake = Faker()\n",
    "    Faker.seed(42)\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "\n",
    "    # Define parameters\n",
    "    categories = ['Electronics', 'Clothing', 'Home', 'Beauty', 'Sports']\n",
    "    start_date = datetime(2024, 11, 29)  # 6 months before May 29, 2025\n",
    "    end_date = datetime(2025, 5, 29)\n",
    "\n",
    "    # Generate product IDs\n",
    "    product_ids = [f'P{str(i).zfill(4)}' for i in range(1, n_products + 1)]\n",
    "\n",
    "    # Generate categories with realistic distribution (e.g., Clothing more common)\n",
    "    category_weights = [0.25, 0.35, 0.20, 0.10, 0.10]  # Weighted probabilities\n",
    "    category_data = random.choices(categories, weights=category_weights, k=n_products)\n",
    "\n",
    "    # Generate product age (days since launch, 0 to 730 days ~ 2 years)\n",
    "    product_age = np.random.exponential(scale=365, size=n_products).astype(int)\n",
    "    product_age = np.clip(product_age, 0, 730)\n",
    "\n",
    "    # Generate price and cost price based on category\n",
    "    price_ranges = {\n",
    "        'Electronics': (50, 1000),\n",
    "        'Clothing': (10, 150),\n",
    "        'Home': (20, 500),\n",
    "        'Beauty': (5, 100),\n",
    "        'Sports': (15, 300)\n",
    "    }\n",
    "    margin_ranges = {\n",
    "        'Electronics': (0.1, 0.3),  \n",
    "        'Clothing': (0.3, 0.6),\n",
    "        'Home': (0.2, 0.5),\n",
    "        'Beauty': (0.4, 0.7),  \n",
    "        'Sports': (0.2, 0.5)\n",
    "    }\n",
    "    prices = []\n",
    "    cost_prices = []\n",
    "    for cat in category_data:\n",
    "        price = random.uniform(price_ranges[cat][0], price_ranges[cat][1])\n",
    "        margin = random.uniform(margin_ranges[cat][0], margin_ranges[cat][1])\n",
    "        cost_price = price * (1 - margin)\n",
    "        prices.append(round(price, 2))\n",
    "        cost_prices.append(round(cost_price, 2))\n",
    "\n",
    "    # Generate units sold with seasonal effects\n",
    "    seasonal_factor = {\n",
    "        'Electronics': [1.5, 1.5, 1.0, 0.8, 0.8, 0.9],  \n",
    "        'Clothing': [1.8, 1.8, 0.9, 0.7, 0.7, 0.8],\n",
    "        'Home': [1.2, 1.2, 1.0, 1.0, 1.0, 1.0],\n",
    "        'Beauty': [1.6, 1.6, 0.8, 0.7, 0.7, 0.8],\n",
    "        'Sports': [0.9, 0.9, 1.2, 1.2, 1.2, 1.0]\n",
    "    }\n",
    "    units_sold = []\n",
    "    for i, cat in enumerate(category_data):\n",
    "        base_sales = np.random.lognormal(mean=4, sigma=1.5)\n",
    "        month_factor = seasonal_factor[cat][random.randint(0, 5)]  \n",
    "        age_factor = 1.0 if product_age[i] > 90 else 0.5  # New products sell less\n",
    "        units = base_sales * month_factor * age_factor\n",
    "        units_sold.append(round(units, 1))\n",
    "    units_sold = np.clip(units_sold, 0, 5000)\n",
    "    # Outliers: viral products\n",
    "    for i in random.sample(range(n_products), 15):\n",
    "        units_sold[i] = random.randint(3500, 5000)\n",
    "\n",
    "    # Generate revenue\n",
    "    revenue = [units * price * random.uniform(0.95, 1.05) for units, price in zip(units_sold, prices)]\n",
    "    revenue = [round(r, 2) for r in revenue]\n",
    "\n",
    "    # Generate stock levels (correlated with units sold)\n",
    "    stock_level = []\n",
    "    for units, cat in zip(units_sold, category_data):\n",
    "        base_stock = np.random.lognormal(mean=5, sigma=1.5)\n",
    "        stock = base_stock * (0.5 if units > 1000 else 1.5 if units < 100 else 1.0)\n",
    "        stock_level.append(int(np.clip(stock, 0, 10000)))\n",
    "    # Outliers: stockouts and overstock\n",
    "    for i in random.sample(range(n_products), 40):\n",
    "        if random.random() < 0.4 and units_sold[i] > 1000:\n",
    "            stock_level[i] = 0  # Stockout for high-demand products\n",
    "        else:\n",
    "            stock_level[i] = random.randint(6000, 10000)  # Overstock\n",
    "\n",
    "    # Generate restock frequency (correlated with units sold and season)\n",
    "    restock_frequency = []\n",
    "    for i, units in enumerate(units_sold):\n",
    "        base_freq = min(round(units / 100), 10)\n",
    "        month_factor = seasonal_factor[category_data[i]][random.randint(0, 5)]\n",
    "        freq = base_freq * month_factor\n",
    "        restock_frequency.append(int(np.clip(np.random.normal(freq, 1), 0, 10)))\n",
    "    # Outliers: no restocks for slow movers\n",
    "    for i in random.sample(range(n_products), 60):\n",
    "        if units_sold[i] < 50:\n",
    "            restock_frequency[i] = 0\n",
    "\n",
    "    # Generate days in inventory (inversely related to units sold)\n",
    "    days_in_inventory = []\n",
    "    for units, age in zip(units_sold, product_age):\n",
    "        if units == 0:\n",
    "            days = 180\n",
    "        else:\n",
    "            days = random.randint(0, min(180, age))\n",
    "            days = days * (1.5 if units < 50 else 0.5 if units > 1000 else 1.0)\n",
    "        days_in_inventory.append(int(days))\n",
    "    # Outliers: old stock\n",
    "    for i in random.sample(range(n_products), 50):\n",
    "        days_in_inventory[i] = random.randint(150, 180)\n",
    "\n",
    "    # Generate last sale date\n",
    "    last_sale_date = []\n",
    "    for units, days, age in zip(units_sold, days_in_inventory, product_age):\n",
    "        if units == 0 or days >= min(180, age):\n",
    "            last_sale_date.append(None)\n",
    "        else:\n",
    "            days_ago = random.randint(0, days)\n",
    "            sale_date = end_date - timedelta(days=days_ago)\n",
    "            last_sale_date.append(sale_date.strftime('%Y-%m-%d'))\n",
    "\n",
    "    # Generate customer ratings (1–5, correlated with units sold)\n",
    "    customer_rating = []\n",
    "    for units in units_sold:\n",
    "        if units == 0:\n",
    "            rating = None\n",
    "        else:\n",
    "            base_rating = min(5, max(1, round(np.random.normal(3.5 + units / 1000, 0.5))))\n",
    "            rating = base_rating if random.random() < 0.8 else None  # 20% missing\n",
    "        customer_rating.append(rating)\n",
    "\n",
    "    # Introduce missing values (tied to product age)\n",
    "    for i in random.sample(range(n_products), 60):\n",
    "        if product_age[i] < 30:  # New products\n",
    "            units_sold[i] = np.nan\n",
    "            revenue[i] = np.nan\n",
    "            last_sale_date[i] = None\n",
    "        elif units_sold[i] < 20:  # Low sellers\n",
    "            last_sale_date[i] = None\n",
    "\n",
    "    # Create DataFrame\n",
    "    data = {\n",
    "        'product_id': product_ids,\n",
    "        'category': category_data,\n",
    "        'product_age': product_age,\n",
    "        'units_sold': units_sold,\n",
    "        'revenue': revenue,\n",
    "        'price': prices,\n",
    "        'cost_price': cost_prices,\n",
    "        'stock_level': stock_level,\n",
    "        'restock_frequency': restock_frequency,\n",
    "        'days_in_inventory': days_in_inventory,\n",
    "        'last_sale_date': last_sale_date,\n",
    "        'customer_rating': customer_rating\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Add new columns as requested\n",
    "    df['profit_margin'] = (df['price'] - df['cost_price']) / df['price']\n",
    "    df['stock_to_sales_ratio'] = df['stock_level'] / (df['units_sold'] + 1)  # +1 to avoid division by zero\n",
    "    df['sales_velocity'] = df['units_sold'] / (df['days_in_inventory'] + 1)  # +1 to avoid division by zero\n",
    "    current_date = datetime(2025, 5, 30)  # Current date: May 30, 2025\n",
    "    df['last_sale_date'] = pd.to_datetime(df['last_sale_date'], errors='coerce')\n",
    "    df['days_since_last_sales'] = (current_date - df['last_sale_date']).dt.days\n",
    "    df['days_since_last_sales'] = df['days_since_last_sales'].fillna(df['days_since_last_sales'].max())  # Fill missing with max\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    df = generate_catalog_data(n_products=1000)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483b786",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "065d619a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>category</th>\n",
       "      <th>product_age</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>price</th>\n",
       "      <th>cost_price</th>\n",
       "      <th>stock_level</th>\n",
       "      <th>restock_frequency</th>\n",
       "      <th>days_in_inventory</th>\n",
       "      <th>last_sale_date</th>\n",
       "      <th>customer_rating</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>stock_to_sales_ratio</th>\n",
       "      <th>sales_velocity</th>\n",
       "      <th>days_since_last_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>Home</td>\n",
       "      <td>171</td>\n",
       "      <td>71.3</td>\n",
       "      <td>5033.17</td>\n",
       "      <td>67.69</td>\n",
       "      <td>40.23</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>170</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.405673</td>\n",
       "      <td>0.373444</td>\n",
       "      <td>0.416959</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>730</td>\n",
       "      <td>6.6</td>\n",
       "      <td>3780.05</td>\n",
       "      <td>567.24</td>\n",
       "      <td>399.58</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.295572</td>\n",
       "      <td>25.789474</td>\n",
       "      <td>0.053226</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>480</td>\n",
       "      <td>77.3</td>\n",
       "      <td>4781.60</td>\n",
       "      <td>60.21</td>\n",
       "      <td>34.96</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.419366</td>\n",
       "      <td>0.293742</td>\n",
       "      <td>1.171212</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>333</td>\n",
       "      <td>204.7</td>\n",
       "      <td>47737.56</td>\n",
       "      <td>230.32</td>\n",
       "      <td>201.66</td>\n",
       "      <td>464</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.124436</td>\n",
       "      <td>2.255712</td>\n",
       "      <td>4.873810</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>Home</td>\n",
       "      <td>61</td>\n",
       "      <td>75.9</td>\n",
       "      <td>33526.10</td>\n",
       "      <td>427.06</td>\n",
       "      <td>283.39</td>\n",
       "      <td>251</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.336416</td>\n",
       "      <td>3.263979</td>\n",
       "      <td>1.518000</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>P0996</td>\n",
       "      <td>Electronics</td>\n",
       "      <td>35</td>\n",
       "      <td>3.0</td>\n",
       "      <td>156.50</td>\n",
       "      <td>51.22</td>\n",
       "      <td>42.14</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>NaT</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.177275</td>\n",
       "      <td>36.250000</td>\n",
       "      <td>0.019231</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>P0997</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>730</td>\n",
       "      <td>17.4</td>\n",
       "      <td>1973.43</td>\n",
       "      <td>112.56</td>\n",
       "      <td>46.07</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.590707</td>\n",
       "      <td>8.206522</td>\n",
       "      <td>0.179381</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>P0998</td>\n",
       "      <td>Clothing</td>\n",
       "      <td>53</td>\n",
       "      <td>18.1</td>\n",
       "      <td>2328.64</td>\n",
       "      <td>133.84</td>\n",
       "      <td>73.89</td>\n",
       "      <td>154</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2025-04-27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.447923</td>\n",
       "      <td>8.062827</td>\n",
       "      <td>0.385106</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>P0999</td>\n",
       "      <td>Sports</td>\n",
       "      <td>730</td>\n",
       "      <td>25.8</td>\n",
       "      <td>3144.19</td>\n",
       "      <td>122.93</td>\n",
       "      <td>78.21</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>NaT</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.363784</td>\n",
       "      <td>7.537313</td>\n",
       "      <td>0.135079</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>P1000</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>215</td>\n",
       "      <td>15.5</td>\n",
       "      <td>223.86</td>\n",
       "      <td>14.64</td>\n",
       "      <td>6.68</td>\n",
       "      <td>456</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.543716</td>\n",
       "      <td>27.636364</td>\n",
       "      <td>0.174157</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id     category  product_age  units_sold   revenue   price  \\\n",
       "0        P0001         Home          171        71.3   5033.17   67.69   \n",
       "1        P0002  Electronics          730         6.6   3780.05  567.24   \n",
       "2        P0003     Clothing          480        77.3   4781.60   60.21   \n",
       "3        P0004  Electronics          333       204.7  47737.56  230.32   \n",
       "4        P0005         Home           61        75.9  33526.10  427.06   \n",
       "..         ...          ...          ...         ...       ...     ...   \n",
       "995      P0996  Electronics           35         3.0    156.50   51.22   \n",
       "996      P0997     Clothing          730        17.4   1973.43  112.56   \n",
       "997      P0998     Clothing           53        18.1   2328.64  133.84   \n",
       "998      P0999       Sports          730        25.8   3144.19  122.93   \n",
       "999      P1000       Beauty          215        15.5    223.86   14.64   \n",
       "\n",
       "     cost_price  stock_level  restock_frequency  days_in_inventory  \\\n",
       "0         40.23           27                  1                170   \n",
       "1        399.58          196                  0                123   \n",
       "2         34.96           23                  0                 65   \n",
       "3        201.66          464                  1                 41   \n",
       "4        283.39          251                  0                 49   \n",
       "..          ...          ...                ...                ...   \n",
       "995       42.14          145                  0                155   \n",
       "996       46.07          151                  0                 96   \n",
       "997       73.89          154                  0                 46   \n",
       "998       78.21          202                  0                190   \n",
       "999        6.68          456                  0                 88   \n",
       "\n",
       "    last_sale_date  customer_rating  profit_margin  stock_to_sales_ratio  \\\n",
       "0       2024-12-23              3.0       0.405673              0.373444   \n",
       "1       2025-03-04              3.0       0.295572             25.789474   \n",
       "2       2025-04-14              3.0       0.419366              0.293742   \n",
       "3       2025-05-11              3.0       0.124436              2.255712   \n",
       "4       2025-05-09              4.0       0.336416              3.263979   \n",
       "..             ...              ...            ...                   ...   \n",
       "995            NaT              4.0       0.177275             36.250000   \n",
       "996     2025-05-24              4.0       0.590707              8.206522   \n",
       "997     2025-04-27              4.0       0.447923              8.062827   \n",
       "998            NaT              3.0       0.363784              7.537313   \n",
       "999     2025-05-16              2.0       0.543716             27.636364   \n",
       "\n",
       "     sales_velocity  days_since_last_sales  \n",
       "0          0.416959                  158.0  \n",
       "1          0.053226                   87.0  \n",
       "2          1.171212                   46.0  \n",
       "3          4.873810                   19.0  \n",
       "4          1.518000                   21.0  \n",
       "..              ...                    ...  \n",
       "995        0.019231                  176.0  \n",
       "996        0.179381                    6.0  \n",
       "997        0.385106                   33.0  \n",
       "998        0.135079                  176.0  \n",
       "999        0.174157                   14.0  \n",
       "\n",
       "[1000 rows x 16 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5eaea5",
   "metadata": {},
   "source": [
    "# Pre-processing the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2917b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data saved to 'preprocessed_catalog_data_original.csv'\n",
      "\n",
      "Sample of preprocessed data:\n",
      "  product_id  product_age  units_sold   revenue     price  cost_price  \\\n",
      "0      P0001    -0.564022   -0.335566 -0.615512 -0.671077   -0.627312   \n",
      "1      P0002     1.727784   -0.861816 -0.670539  1.359982    1.141727   \n",
      "2      P0003     0.702826   -0.286764 -0.626559 -0.701489   -0.653256   \n",
      "3      P0004     0.100151    0.749470  1.259712 -0.009859    0.167389   \n",
      "4      P0005    -1.015004   -0.298151  0.635662  0.790042    0.569737   \n",
      "\n",
      "   stock_level  restock_frequency  days_in_inventory last_sale_date  \\\n",
      "0    -0.855518          -0.081042           1.296641     2024-12-23   \n",
      "1    -0.466750          -0.690378           0.588407     2025-03-04   \n",
      "2    -0.864719          -0.690378          -0.285584     2025-04-14   \n",
      "3     0.149757          -0.081042          -0.647236     2025-05-11   \n",
      "4    -0.340228          -0.690378          -0.526685     2025-05-09   \n",
      "\n",
      "   customer_rating  profit_margin  stock_to_sales_ratio  sales_velocity  \\\n",
      "0        -1.193088       0.251921             -0.751089       -0.682131   \n",
      "1        -1.193088      -0.536336              0.973685       -0.785109   \n",
      "2        -1.193088       0.349952             -0.756498       -0.468591   \n",
      "3        -1.193088      -1.761563             -0.623355        0.579670   \n",
      "4         0.494448      -0.243912             -0.554932       -0.370410   \n",
      "\n",
      "   days_since_last_sales  category_Beauty  category_Clothing  \\\n",
      "0               1.513440              0.0                0.0   \n",
      "1               0.405253              0.0                0.0   \n",
      "2              -0.234686              0.0                1.0   \n",
      "3              -0.656109              0.0                0.0   \n",
      "4              -0.624893              0.0                0.0   \n",
      "\n",
      "   category_Electronics  category_Home  category_Sports  \n",
      "0                   0.0            1.0              0.0  \n",
      "1                   1.0            0.0              0.0  \n",
      "2                   0.0            0.0              0.0  \n",
      "3                   1.0            0.0              0.0  \n",
      "4                   0.0            1.0              0.0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    # Create a copy to avoid modifying the original\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Sales-related: assume 0 for no sales\n",
    "    df_clean['units_sold'] = df_clean['units_sold'].fillna(0)\n",
    "    df_clean['revenue'] = df_clean['revenue'].fillna(0)\n",
    "    \n",
    "    # Customer rating: fill with median to preserve distribution\n",
    "    df_clean['customer_rating'] = df_clean['customer_rating'].fillna(df_clean['customer_rating'].median())\n",
    "    \n",
    "    # Numeric columns: fill with median for robustness\n",
    "    for col in ['stock_level', 'restock_frequency', 'days_in_inventory', \n",
    "                'stock_to_sales_ratio', 'sales_velocity', 'days_since_last_sales']:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "    \n",
    "    # Last sale date: if missing, assume no sales, set to start date (Nov 29, 2024)\n",
    "    start_date = datetime(2024, 11, 29)\n",
    "    df_clean['last_sale_date'] = df_clean['last_sale_date'].fillna(start_date)\n",
    "    # Recalculate days_since_last_sales after filling\n",
    "    current_date = datetime(2025, 5, 30)\n",
    "    df_clean['days_since_last_sales'] = (current_date - df_clean['last_sale_date']).dt.days\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def handle_outliers(df):\n",
    "    # Create a copy to avoid modifying the input\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Numeric columns to check for outliers\n",
    "    numeric_cols = ['units_sold', 'revenue', 'stock_level', 'restock_frequency', \n",
    "                    'days_in_inventory', 'profit_margin', 'stock_to_sales_ratio', \n",
    "                    'sales_velocity', 'days_since_last_sales']\n",
    "    \n",
    "    # IQR method: cap outliers at Q1 - 1.5*IQR and Q3 + 1.5*IQR\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df_clean[col].quantile(0.25)\n",
    "        Q3 = df_clean[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def encode_categorical(df):\n",
    "    # Create a copy to avoid modifying the input\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # One-hot encode the 'category' column\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "    category_encoded = encoder.fit_transform(df_clean[['category']])\n",
    "    \n",
    "    # Create column names for encoded features\n",
    "    encoded_columns = [f\"category_{cat}\" for cat in encoder.categories_[0]]\n",
    "    \n",
    "    # Convert encoded data to DataFrame and join with original\n",
    "    category_df = pd.DataFrame(category_encoded, columns=encoded_columns, index=df_clean.index)\n",
    "    df_clean = pd.concat([df_clean, category_df], axis=1)\n",
    "    \n",
    "    # Drop original 'category' column to avoid redundancy\n",
    "    df_clean = df_clean.drop(columns=['category'])\n",
    "    \n",
    "    return df_clean, encoder\n",
    "\n",
    "def scale_features(df):\n",
    "    # Create a copy to avoid modifying the input\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Numeric columns to scale\n",
    "    numeric_cols = ['product_age', 'units_sold', 'revenue', 'price', 'cost_price', \n",
    "                    'stock_level', 'restock_frequency', 'days_in_inventory', \n",
    "                    'customer_rating', 'profit_margin', 'stock_to_sales_ratio', \n",
    "                    'sales_velocity', 'days_since_last_sales']\n",
    "    \n",
    "    # Apply StandardScaler (mean=0, std=1)\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled[numeric_cols] = scaler.fit_transform(df_scaled[numeric_cols])\n",
    "    \n",
    "    # One-hot encoded columns (category_*) and product_id, last_sale_date not scaled\n",
    "    return df_scaled, scaler\n",
    "\n",
    "\n",
    "def preprocess_pipeline(n_products=1000, output_file='preprocessed_catalog_data_original.csv'):\n",
    "    df = generate_catalog_data(n_products)\n",
    "    df_preprocessed = df.copy()\n",
    "    df_preprocessed = handle_missing_values(df_preprocessed)\n",
    "    df_preprocessed = handle_outliers(df_preprocessed)\n",
    "    df_preprocessed, encoder = encode_categorical(df_preprocessed)\n",
    "    df_preprocessed, scaler = scale_features(df_preprocessed)\n",
    "    df_preprocessed.to_csv(output_file, index=False)\n",
    "    print(f\"Preprocessed data saved to '{output_file}'\")\n",
    "    print(\"\\nSample of preprocessed data:\")\n",
    "    print(df_preprocessed.head())\n",
    "    return df, df_preprocessed, encoder, scaler\n",
    "# Run the pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    df, df_preprocessed_original, encoder, scaler = preprocess_pipeline(n_products=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d1c419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_age</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>price</th>\n",
       "      <th>cost_price</th>\n",
       "      <th>stock_level</th>\n",
       "      <th>restock_frequency</th>\n",
       "      <th>days_in_inventory</th>\n",
       "      <th>last_sale_date</th>\n",
       "      <th>customer_rating</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>stock_to_sales_ratio</th>\n",
       "      <th>sales_velocity</th>\n",
       "      <th>days_since_last_sales</th>\n",
       "      <th>category_Beauty</th>\n",
       "      <th>category_Clothing</th>\n",
       "      <th>category_Electronics</th>\n",
       "      <th>category_Home</th>\n",
       "      <th>category_Sports</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>-0.564022</td>\n",
       "      <td>-0.335566</td>\n",
       "      <td>-0.615512</td>\n",
       "      <td>-0.671077</td>\n",
       "      <td>-0.627312</td>\n",
       "      <td>-0.855518</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>1.296641</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>-1.193088</td>\n",
       "      <td>0.251921</td>\n",
       "      <td>-0.751089</td>\n",
       "      <td>-0.682131</td>\n",
       "      <td>1.513440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.861816</td>\n",
       "      <td>-0.670539</td>\n",
       "      <td>1.359982</td>\n",
       "      <td>1.141727</td>\n",
       "      <td>-0.466750</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.588407</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>-1.193088</td>\n",
       "      <td>-0.536336</td>\n",
       "      <td>0.973685</td>\n",
       "      <td>-0.785109</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>0.702826</td>\n",
       "      <td>-0.286764</td>\n",
       "      <td>-0.626559</td>\n",
       "      <td>-0.701489</td>\n",
       "      <td>-0.653256</td>\n",
       "      <td>-0.864719</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.285584</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>-1.193088</td>\n",
       "      <td>0.349952</td>\n",
       "      <td>-0.756498</td>\n",
       "      <td>-0.468591</td>\n",
       "      <td>-0.234686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>0.100151</td>\n",
       "      <td>0.749470</td>\n",
       "      <td>1.259712</td>\n",
       "      <td>-0.009859</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.149757</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>-0.647236</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>-1.193088</td>\n",
       "      <td>-1.761563</td>\n",
       "      <td>-0.623355</td>\n",
       "      <td>0.579670</td>\n",
       "      <td>-0.656109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>-1.015004</td>\n",
       "      <td>-0.298151</td>\n",
       "      <td>0.635662</td>\n",
       "      <td>0.790042</td>\n",
       "      <td>0.569737</td>\n",
       "      <td>-0.340228</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.526685</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>0.494448</td>\n",
       "      <td>-0.243912</td>\n",
       "      <td>-0.554932</td>\n",
       "      <td>-0.370410</td>\n",
       "      <td>-0.624893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>P0996</td>\n",
       "      <td>-1.121599</td>\n",
       "      <td>-0.891098</td>\n",
       "      <td>-0.829655</td>\n",
       "      <td>-0.738040</td>\n",
       "      <td>-0.617909</td>\n",
       "      <td>-0.584071</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>1.070609</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>0.494448</td>\n",
       "      <td>-1.383269</td>\n",
       "      <td>1.683554</td>\n",
       "      <td>-0.794734</td>\n",
       "      <td>1.888038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>P0997</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.773972</td>\n",
       "      <td>-0.749871</td>\n",
       "      <td>-0.488645</td>\n",
       "      <td>-0.598562</td>\n",
       "      <td>-0.570268</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.181549</td>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>0.494448</td>\n",
       "      <td>1.576651</td>\n",
       "      <td>-0.219523</td>\n",
       "      <td>-0.749393</td>\n",
       "      <td>-0.859017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>P0998</td>\n",
       "      <td>-1.047802</td>\n",
       "      <td>-0.768279</td>\n",
       "      <td>-0.734273</td>\n",
       "      <td>-0.402125</td>\n",
       "      <td>-0.461608</td>\n",
       "      <td>-0.563367</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.571892</td>\n",
       "      <td>2025-04-27</td>\n",
       "      <td>0.494448</td>\n",
       "      <td>0.554404</td>\n",
       "      <td>-0.229274</td>\n",
       "      <td>-0.691149</td>\n",
       "      <td>-0.437593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>P0999</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.705649</td>\n",
       "      <td>-0.698460</td>\n",
       "      <td>-0.446483</td>\n",
       "      <td>-0.440341</td>\n",
       "      <td>-0.452948</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>1.598017</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>-1.193088</td>\n",
       "      <td>-0.047976</td>\n",
       "      <td>-0.264937</td>\n",
       "      <td>-0.761935</td>\n",
       "      <td>1.888038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>P1000</td>\n",
       "      <td>-0.383629</td>\n",
       "      <td>-0.789426</td>\n",
       "      <td>-0.826697</td>\n",
       "      <td>-0.886766</td>\n",
       "      <td>-0.792475</td>\n",
       "      <td>0.131354</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>-2.880624</td>\n",
       "      <td>1.240222</td>\n",
       "      <td>1.099018</td>\n",
       "      <td>-0.750872</td>\n",
       "      <td>-0.734150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id  product_age  units_sold   revenue     price  cost_price  \\\n",
       "0        P0001    -0.564022   -0.335566 -0.615512 -0.671077   -0.627312   \n",
       "1        P0002     1.727784   -0.861816 -0.670539  1.359982    1.141727   \n",
       "2        P0003     0.702826   -0.286764 -0.626559 -0.701489   -0.653256   \n",
       "3        P0004     0.100151    0.749470  1.259712 -0.009859    0.167389   \n",
       "4        P0005    -1.015004   -0.298151  0.635662  0.790042    0.569737   \n",
       "..         ...          ...         ...       ...       ...         ...   \n",
       "995      P0996    -1.121599   -0.891098 -0.829655 -0.738040   -0.617909   \n",
       "996      P0997     1.727784   -0.773972 -0.749871 -0.488645   -0.598562   \n",
       "997      P0998    -1.047802   -0.768279 -0.734273 -0.402125   -0.461608   \n",
       "998      P0999     1.727784   -0.705649 -0.698460 -0.446483   -0.440341   \n",
       "999      P1000    -0.383629   -0.789426 -0.826697 -0.886766   -0.792475   \n",
       "\n",
       "     stock_level  restock_frequency  days_in_inventory last_sale_date  \\\n",
       "0      -0.855518          -0.081042           1.296641     2024-12-23   \n",
       "1      -0.466750          -0.690378           0.588407     2025-03-04   \n",
       "2      -0.864719          -0.690378          -0.285584     2025-04-14   \n",
       "3       0.149757          -0.081042          -0.647236     2025-05-11   \n",
       "4      -0.340228          -0.690378          -0.526685     2025-05-09   \n",
       "..           ...                ...                ...            ...   \n",
       "995    -0.584071          -0.690378           1.070609     2024-11-29   \n",
       "996    -0.570268          -0.690378           0.181549     2025-05-24   \n",
       "997    -0.563367          -0.690378          -0.571892     2025-04-27   \n",
       "998    -0.452948          -0.690378           1.598017     2024-11-29   \n",
       "999     0.131354          -0.690378           0.060999     2025-05-16   \n",
       "\n",
       "     customer_rating  profit_margin  stock_to_sales_ratio  sales_velocity  \\\n",
       "0          -1.193088       0.251921             -0.751089       -0.682131   \n",
       "1          -1.193088      -0.536336              0.973685       -0.785109   \n",
       "2          -1.193088       0.349952             -0.756498       -0.468591   \n",
       "3          -1.193088      -1.761563             -0.623355        0.579670   \n",
       "4           0.494448      -0.243912             -0.554932       -0.370410   \n",
       "..               ...            ...                   ...             ...   \n",
       "995         0.494448      -1.383269              1.683554       -0.794734   \n",
       "996         0.494448       1.576651             -0.219523       -0.749393   \n",
       "997         0.494448       0.554404             -0.229274       -0.691149   \n",
       "998        -1.193088      -0.047976             -0.264937       -0.761935   \n",
       "999        -2.880624       1.240222              1.099018       -0.750872   \n",
       "\n",
       "     days_since_last_sales  category_Beauty  category_Clothing  \\\n",
       "0                 1.513440              0.0                0.0   \n",
       "1                 0.405253              0.0                0.0   \n",
       "2                -0.234686              0.0                1.0   \n",
       "3                -0.656109              0.0                0.0   \n",
       "4                -0.624893              0.0                0.0   \n",
       "..                     ...              ...                ...   \n",
       "995               1.888038              0.0                0.0   \n",
       "996              -0.859017              0.0                1.0   \n",
       "997              -0.437593              0.0                1.0   \n",
       "998               1.888038              0.0                0.0   \n",
       "999              -0.734150              1.0                0.0   \n",
       "\n",
       "     category_Electronics  category_Home  category_Sports  \n",
       "0                     0.0            1.0              0.0  \n",
       "1                     1.0            0.0              0.0  \n",
       "2                     0.0            0.0              0.0  \n",
       "3                     1.0            0.0              0.0  \n",
       "4                     0.0            1.0              0.0  \n",
       "..                    ...            ...              ...  \n",
       "995                   1.0            0.0              0.0  \n",
       "996                   0.0            0.0              0.0  \n",
       "997                   0.0            0.0              0.0  \n",
       "998                   0.0            0.0              1.0  \n",
       "999                   0.0            0.0              0.0  \n",
       "\n",
       "[1000 rows x 20 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        df_preprocessed_original = pd.read_csv('preprocessed_catalog_data_original.csv')\n",
    "\n",
    "df_preprocessed_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85586872",
   "metadata": {},
   "source": [
    "### The data is cleaned and scaled now with added new features that will help us classify into the 4 categories.\n",
    "### (The reason for these actions has been described in the Documentation of the Assignment (e.g : Why do we have to scale the data? What happens if we don't?))\n",
    "\n",
    "### Now, we will focus on Analysis this data to come up with a recommendation system based on the necessary/important features.\n",
    "\n",
    "### So, the next question is : Which features are the most important one?\n",
    "\n",
    "### Let's find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48c07729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Random Forest analysis pipeline...\n",
      "Creating proxy labels...\n",
      "Proxy labels created successfully.\n",
      "Analyzing feature importance...\n",
      "\n",
      "Feature Importance (Top 10):\n",
      "                  Feature  Importance\n",
      "11         sales_velocity    0.246322\n",
      "10   stock_to_sales_ratio    0.149267\n",
      "1              units_sold    0.148739\n",
      "2                 revenue    0.134174\n",
      "7       days_in_inventory    0.077597\n",
      "6       restock_frequency    0.057702\n",
      "5             stock_level    0.050790\n",
      "12  days_since_last_sales    0.036553\n",
      "3                   price    0.024385\n",
      "4              cost_price    0.023829\n",
      "\n",
      "Train Accuracy: 0.999\n",
      "Test Accuracy: 0.980\n",
      "Cross-Validation Accuracy: 0.968 (+/- 0.016)\n",
      "Labeled data saved to 'rf_labeled_catalog_data.csv'\n",
      "\n",
      "Sample of labeled data:\n",
      "  product_id  units_sold   revenue  stock_level recommendation\n",
      "0      P0001   -0.335566 -0.615512    -0.855518        Promote\n",
      "1      P0002   -0.861816 -0.670539    -0.466750        Reprice\n",
      "2      P0003   -0.286764 -0.626559    -0.864719        Promote\n",
      "3      P0004    0.749470  1.259712     0.149757     Prioritize\n",
      "4      P0005   -0.298151  0.635662    -0.340228        Promote\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "def create_proxy_labels(df):\n",
    "    print(\"Creating proxy labels...\")\n",
    "    try:\n",
    "        df_labeled = df.copy()\n",
    "        low_sales = df['units_sold'].quantile(0.25)\n",
    "        low_revenue = df['revenue'].quantile(0.25)\n",
    "        high_days_inv = df['days_in_inventory'].quantile(0.75)\n",
    "        high_days_since = df['days_since_last_sales'].quantile(0.75)\n",
    "        high_sales = df['units_sold'].quantile(0.75)\n",
    "        high_revenue = df['revenue'].quantile(0.75)\n",
    "        high_velocity = df['sales_velocity'].quantile(0.75)\n",
    "        high_stock_ratio = df['stock_to_sales_ratio'].quantile(0.75)\n",
    "        low_velocity = df['sales_velocity'].quantile(0.25)\n",
    "        conditions = [\n",
    "            (df_labeled['units_sold'] < low_sales) & \n",
    "            (df_labeled['revenue'] < low_revenue) & \n",
    "            (df_labeled['days_in_inventory'] > high_days_inv) & \n",
    "            (df_labeled['days_since_last_sales'] > high_days_since),\n",
    "            (df_labeled['units_sold'] > high_sales) & \n",
    "            (df_labeled['revenue'] > high_revenue) & \n",
    "            (df_labeled['sales_velocity'] > high_velocity),\n",
    "            (df_labeled['stock_to_sales_ratio'] > high_stock_ratio) & \n",
    "            (df_labeled['sales_velocity'] < low_velocity),\n",
    "        ]\n",
    "        choices = ['Discontinue', 'Prioritize', 'Reprice']\n",
    "        df_labeled['recommendation'] = np.select(conditions, choices, default='Promote')\n",
    "        print(\"Proxy labels created successfully.\")\n",
    "        return df_labeled\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_proxy_labels: {e}\")\n",
    "        raise\n",
    "\n",
    "def analyze_feature_importance(df):\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    try:\n",
    "        # Verify required columns\n",
    "        required_cols = ['product_id', 'last_sale_date', 'recommendation']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        X = df.drop(columns=['product_id', 'last_sale_date', 'recommendation'])\n",
    "        y = df['recommendation']\n",
    "        \n",
    "        # Verify data integrity\n",
    "        if X.empty or y.empty:\n",
    "            raise ValueError(\"Feature matrix or target is empty\")\n",
    "        if X.isna().any().any() or y.isna().any():\n",
    "            raise ValueError(\"NaN values detected in features or target\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=10, min_samples_split=5, \n",
    "                                    min_samples_leaf=2, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        importance = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': rf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(\"\\nFeature Importance (Top 10):\")\n",
    "        print(importance.head(10))\n",
    "        train_score = rf.score(X_train, y_train)\n",
    "        test_score = rf.score(X_test, y_test)\n",
    "        cv_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "        print(f\"\\nTrain Accuracy: {train_score:.3f}\")\n",
    "        print(f\"Test Accuracy: {test_score:.3f}\")\n",
    "        print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        return importance, rf\n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_feature_importance: {e}\")\n",
    "        raise\n",
    "\n",
    "def rf_analysis_pipeline(df_preprocessed):\n",
    "\n",
    "    # Can remove this, this is just to check whether the columns are there or not.\n",
    "    print(\"Starting Random Forest analysis pipeline...\")\n",
    "    try:\n",
    "        # Verify df_preprocessed columns\n",
    "        expected_cols = ['product_id', 'last_sale_date', 'product_age', 'units_sold', 'revenue', \n",
    "                         'price', 'cost_price', 'stock_level', 'restock_frequency', \n",
    "                         'days_in_inventory', 'customer_rating', 'profit_margin', \n",
    "                         'stock_to_sales_ratio', 'sales_velocity', 'days_since_last_sales',\n",
    "                         'category_Electronics', 'category_Clothing', 'category_Home', \n",
    "                         'category_Beauty', 'category_Sports']\n",
    "        missing_cols = [col for col in expected_cols if col not in df_preprocessed.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in df_preprocessed: {missing_cols}\")\n",
    "        \n",
    "        df_rf = df_preprocessed.copy()\n",
    "        df_rf = create_proxy_labels(df_rf)\n",
    "        importance, model = analyze_feature_importance(df_rf)\n",
    "        df_rf.to_csv('rf_labeled_catalog_data.csv', index=False)\n",
    "        print(\"Labeled data saved to 'rf_labeled_catalog_data.csv'\")\n",
    "        print(\"\\nSample of labeled data:\")\n",
    "        print(df_rf[['product_id', 'units_sold', 'revenue', 'stock_level', 'recommendation']].head())\n",
    "        return df_rf, importance, model\n",
    "    except Exception as e:\n",
    "        print(f\"Error in rf_analysis_pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "        df_preprocessed_original = pd.read_csv('preprocessed_catalog_data_original.csv')\n",
    "        \n",
    "        # Run Random Forest analysis\n",
    "        df_rf, importance, model = rf_analysis_pipeline(df_preprocessed_original)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3fa9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_age</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>price</th>\n",
       "      <th>cost_price</th>\n",
       "      <th>stock_level</th>\n",
       "      <th>restock_frequency</th>\n",
       "      <th>days_in_inventory</th>\n",
       "      <th>last_sale_date</th>\n",
       "      <th>...</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>stock_to_sales_ratio</th>\n",
       "      <th>sales_velocity</th>\n",
       "      <th>days_since_last_sales</th>\n",
       "      <th>category_Beauty</th>\n",
       "      <th>category_Clothing</th>\n",
       "      <th>category_Electronics</th>\n",
       "      <th>category_Home</th>\n",
       "      <th>category_Sports</th>\n",
       "      <th>recommendation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>-0.564022</td>\n",
       "      <td>-0.335566</td>\n",
       "      <td>-0.615512</td>\n",
       "      <td>-0.671077</td>\n",
       "      <td>-0.627312</td>\n",
       "      <td>-0.855518</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>1.296641</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251921</td>\n",
       "      <td>-0.751089</td>\n",
       "      <td>-0.682131</td>\n",
       "      <td>1.513440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.861816</td>\n",
       "      <td>-0.670539</td>\n",
       "      <td>1.359982</td>\n",
       "      <td>1.141727</td>\n",
       "      <td>-0.466750</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.588407</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.536336</td>\n",
       "      <td>0.973685</td>\n",
       "      <td>-0.785109</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Reprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>0.702826</td>\n",
       "      <td>-0.286764</td>\n",
       "      <td>-0.626559</td>\n",
       "      <td>-0.701489</td>\n",
       "      <td>-0.653256</td>\n",
       "      <td>-0.864719</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.285584</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349952</td>\n",
       "      <td>-0.756498</td>\n",
       "      <td>-0.468591</td>\n",
       "      <td>-0.234686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>0.100151</td>\n",
       "      <td>0.749470</td>\n",
       "      <td>1.259712</td>\n",
       "      <td>-0.009859</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.149757</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>-0.647236</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.761563</td>\n",
       "      <td>-0.623355</td>\n",
       "      <td>0.579670</td>\n",
       "      <td>-0.656109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Prioritize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>-1.015004</td>\n",
       "      <td>-0.298151</td>\n",
       "      <td>0.635662</td>\n",
       "      <td>0.790042</td>\n",
       "      <td>0.569737</td>\n",
       "      <td>-0.340228</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.526685</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243912</td>\n",
       "      <td>-0.554932</td>\n",
       "      <td>-0.370410</td>\n",
       "      <td>-0.624893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>P0996</td>\n",
       "      <td>-1.121599</td>\n",
       "      <td>-0.891098</td>\n",
       "      <td>-0.829655</td>\n",
       "      <td>-0.738040</td>\n",
       "      <td>-0.617909</td>\n",
       "      <td>-0.584071</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>1.070609</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.383269</td>\n",
       "      <td>1.683554</td>\n",
       "      <td>-0.794734</td>\n",
       "      <td>1.888038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Discontinue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>P0997</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.773972</td>\n",
       "      <td>-0.749871</td>\n",
       "      <td>-0.488645</td>\n",
       "      <td>-0.598562</td>\n",
       "      <td>-0.570268</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.181549</td>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>...</td>\n",
       "      <td>1.576651</td>\n",
       "      <td>-0.219523</td>\n",
       "      <td>-0.749393</td>\n",
       "      <td>-0.859017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>P0998</td>\n",
       "      <td>-1.047802</td>\n",
       "      <td>-0.768279</td>\n",
       "      <td>-0.734273</td>\n",
       "      <td>-0.402125</td>\n",
       "      <td>-0.461608</td>\n",
       "      <td>-0.563367</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.571892</td>\n",
       "      <td>2025-04-27</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554404</td>\n",
       "      <td>-0.229274</td>\n",
       "      <td>-0.691149</td>\n",
       "      <td>-0.437593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>P0999</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.705649</td>\n",
       "      <td>-0.698460</td>\n",
       "      <td>-0.446483</td>\n",
       "      <td>-0.440341</td>\n",
       "      <td>-0.452948</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>1.598017</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047976</td>\n",
       "      <td>-0.264937</td>\n",
       "      <td>-0.761935</td>\n",
       "      <td>1.888038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>P1000</td>\n",
       "      <td>-0.383629</td>\n",
       "      <td>-0.789426</td>\n",
       "      <td>-0.826697</td>\n",
       "      <td>-0.886766</td>\n",
       "      <td>-0.792475</td>\n",
       "      <td>0.131354</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.240222</td>\n",
       "      <td>1.099018</td>\n",
       "      <td>-0.750872</td>\n",
       "      <td>-0.734150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Reprice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id  product_age  units_sold   revenue     price  cost_price  \\\n",
       "0        P0001    -0.564022   -0.335566 -0.615512 -0.671077   -0.627312   \n",
       "1        P0002     1.727784   -0.861816 -0.670539  1.359982    1.141727   \n",
       "2        P0003     0.702826   -0.286764 -0.626559 -0.701489   -0.653256   \n",
       "3        P0004     0.100151    0.749470  1.259712 -0.009859    0.167389   \n",
       "4        P0005    -1.015004   -0.298151  0.635662  0.790042    0.569737   \n",
       "..         ...          ...         ...       ...       ...         ...   \n",
       "995      P0996    -1.121599   -0.891098 -0.829655 -0.738040   -0.617909   \n",
       "996      P0997     1.727784   -0.773972 -0.749871 -0.488645   -0.598562   \n",
       "997      P0998    -1.047802   -0.768279 -0.734273 -0.402125   -0.461608   \n",
       "998      P0999     1.727784   -0.705649 -0.698460 -0.446483   -0.440341   \n",
       "999      P1000    -0.383629   -0.789426 -0.826697 -0.886766   -0.792475   \n",
       "\n",
       "     stock_level  restock_frequency  days_in_inventory last_sale_date  ...  \\\n",
       "0      -0.855518          -0.081042           1.296641     2024-12-23  ...   \n",
       "1      -0.466750          -0.690378           0.588407     2025-03-04  ...   \n",
       "2      -0.864719          -0.690378          -0.285584     2025-04-14  ...   \n",
       "3       0.149757          -0.081042          -0.647236     2025-05-11  ...   \n",
       "4      -0.340228          -0.690378          -0.526685     2025-05-09  ...   \n",
       "..           ...                ...                ...            ...  ...   \n",
       "995    -0.584071          -0.690378           1.070609     2024-11-29  ...   \n",
       "996    -0.570268          -0.690378           0.181549     2025-05-24  ...   \n",
       "997    -0.563367          -0.690378          -0.571892     2025-04-27  ...   \n",
       "998    -0.452948          -0.690378           1.598017     2024-11-29  ...   \n",
       "999     0.131354          -0.690378           0.060999     2025-05-16  ...   \n",
       "\n",
       "     profit_margin  stock_to_sales_ratio  sales_velocity  \\\n",
       "0         0.251921             -0.751089       -0.682131   \n",
       "1        -0.536336              0.973685       -0.785109   \n",
       "2         0.349952             -0.756498       -0.468591   \n",
       "3        -1.761563             -0.623355        0.579670   \n",
       "4        -0.243912             -0.554932       -0.370410   \n",
       "..             ...                   ...             ...   \n",
       "995      -1.383269              1.683554       -0.794734   \n",
       "996       1.576651             -0.219523       -0.749393   \n",
       "997       0.554404             -0.229274       -0.691149   \n",
       "998      -0.047976             -0.264937       -0.761935   \n",
       "999       1.240222              1.099018       -0.750872   \n",
       "\n",
       "     days_since_last_sales  category_Beauty  category_Clothing  \\\n",
       "0                 1.513440              0.0                0.0   \n",
       "1                 0.405253              0.0                0.0   \n",
       "2                -0.234686              0.0                1.0   \n",
       "3                -0.656109              0.0                0.0   \n",
       "4                -0.624893              0.0                0.0   \n",
       "..                     ...              ...                ...   \n",
       "995               1.888038              0.0                0.0   \n",
       "996              -0.859017              0.0                1.0   \n",
       "997              -0.437593              0.0                1.0   \n",
       "998               1.888038              0.0                0.0   \n",
       "999              -0.734150              1.0                0.0   \n",
       "\n",
       "     category_Electronics  category_Home  category_Sports  recommendation  \n",
       "0                     0.0            1.0              0.0         Promote  \n",
       "1                     1.0            0.0              0.0         Reprice  \n",
       "2                     0.0            0.0              0.0         Promote  \n",
       "3                     1.0            0.0              0.0      Prioritize  \n",
       "4                     0.0            1.0              0.0         Promote  \n",
       "..                    ...            ...              ...             ...  \n",
       "995                   1.0            0.0              0.0     Discontinue  \n",
       "996                   0.0            0.0              0.0         Promote  \n",
       "997                   0.0            0.0              0.0         Promote  \n",
       "998                   0.0            0.0              1.0         Promote  \n",
       "999                   0.0            0.0              0.0         Reprice  \n",
       "\n",
       "[1000 rows x 21 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb7825b",
   "metadata": {},
   "source": [
    "## We used Random Forest. Now we will use K-Means Clustering to cluster the products into 4 categories.\n",
    "## Let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d3a10d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-means analysis pipeline at 2025-05-31 21:08:33\n",
      "Applying K-means clustering...\n",
      "Silhouette Score: 0.194 (range: -1 to 1, higher is better)\n",
      "Davies-Bouldin Index: 1.688 (lower is better)\n",
      "Calinski-Harabasz Index: 213.671 (higher is better)\n",
      "Analyzing feature importance...\n",
      "\n",
      "Feature Importance (Top 10) from K-means Clustering:\n",
      "                 Feature  Importance\n",
      "4             cost_price    0.150595\n",
      "3                  price    0.142813\n",
      "1             units_sold    0.128042\n",
      "10  stock_to_sales_ratio    0.115394\n",
      "6      restock_frequency    0.111813\n",
      "2                revenue    0.092155\n",
      "11        sales_velocity    0.075421\n",
      "5            stock_level    0.074467\n",
      "9          profit_margin    0.055162\n",
      "15  category_Electronics    0.017768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satti\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1382: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=4.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance plot saved to 'feature_importance_kmeans.png'\n",
      "Mapping clusters to recommendations...\n",
      "Cluster mapping completed.\n",
      "Clustered data saved to 'kmeans_clustered_catalog_data.csv'\n",
      "\n",
      "Sample of clustered data:\n",
      "  product_id  units_sold   revenue  stock_level  cluster recommendation\n",
      "0      P0001   -0.335566 -0.615512    -0.855518        3        Promote\n",
      "1      P0002   -0.861816 -0.670539    -0.466750        1        Promote\n",
      "2      P0003   -0.286764 -0.626559    -0.864719        3        Promote\n",
      "3      P0004    0.749470  1.259712     0.149757        2     Prioritize\n",
      "4      P0005   -0.298151  0.635662    -0.340228        3        Promote\n",
      "\n",
      "Cluster Distribution:\n",
      "recommendation\n",
      "Promote       807\n",
      "Prioritize    193\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def apply_kmeans_clustering(df, n_clusters=4):\n",
    "    print(\"Applying K-means clustering...\")\n",
    "    try:\n",
    "        # Verify required columns\n",
    "        expected_cols = ['product_id', 'last_sale_date', 'product_age', 'units_sold', 'revenue', \n",
    "                         'price', 'cost_price', 'stock_level', 'restock_frequency', \n",
    "                         'days_in_inventory', 'customer_rating', 'profit_margin', \n",
    "                         'stock_to_sales_ratio', 'sales_velocity', 'days_since_last_sales',\n",
    "                         'category_Electronics', 'category_Clothing', 'category_Home', \n",
    "                         'category_Beauty', 'category_Sports']\n",
    "        missing_cols = [col for col in expected_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        # Features for clustering\n",
    "        X = df.drop(columns=['product_id', 'last_sale_date'])\n",
    "        \n",
    "        # Verify data integrity\n",
    "        if X.empty:\n",
    "            raise ValueError(\"Feature matrix is empty\")\n",
    "        if X.isna().any().any():\n",
    "            raise ValueError(\"NaN values detected in features\")\n",
    "        \n",
    "        # Apply K-means\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        df['cluster'] = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Compute evaluation metrics\n",
    "        silhouette = silhouette_score(X, df['cluster'])\n",
    "        db_score = davies_bouldin_score(X, df['cluster'])\n",
    "        ch_score = calinski_harabasz_score(X, df['cluster'])\n",
    "        print(f\"Silhouette Score: {silhouette:.3f} (range: -1 to 1, higher is better)\")\n",
    "        print(f\"Davies-Bouldin Index: {db_score:.3f} (lower is better)\")\n",
    "        print(f\"Calinski-Harabasz Index: {ch_score:.3f} (higher is better)\")\n",
    "        \n",
    "        return df, kmeans, silhouette\n",
    "    except Exception as e:\n",
    "        print(f\"Error in apply_kmeans_clustering: {e}\")\n",
    "        raise\n",
    "\n",
    "def analyze_feature_importance(df):\n",
    "    print(\"Analyzing feature importance...\")\n",
    "    try:\n",
    "        X = df.drop(columns=['product_id', 'last_sale_date', 'cluster'])\n",
    "        cluster_means = df.groupby('cluster')[X.columns].mean()\n",
    "        between_cluster_variance = cluster_means.var()\n",
    "        importance = pd.DataFrame({\n",
    "            'Feature': between_cluster_variance.index,\n",
    "            'Importance': between_cluster_variance.values / between_cluster_variance.sum()\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(\"\\nFeature Importance (Top 10) from K-means Clustering:\")\n",
    "        print(importance.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance.head(10))\n",
    "        plt.title('Top 10 Feature Importance from K-means Clustering')\n",
    "        plt.xlabel('Normalized Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance_kmeans.png')\n",
    "        plt.close()\n",
    "        print(\"Feature importance plot saved to 'feature_importance_kmeans.png'\")\n",
    "        \n",
    "        return importance, cluster_means\n",
    "    except Exception as e:\n",
    "        print(f\"Error in analyze_feature_importance: {e}\")\n",
    "        raise\n",
    "\n",
    "def map_clusters_to_recommendations(df, cluster_means):\n",
    "    print(\"Mapping clusters to recommendations...\")\n",
    "    try:\n",
    "        df_mapped = df.copy()\n",
    "        labels = {}\n",
    "        for cluster in cluster_means.index:\n",
    "            mean_velocity = cluster_means.loc[cluster, 'sales_velocity']\n",
    "            mean_sales = cluster_means.loc[cluster, 'units_sold']\n",
    "            mean_stock_ratio = cluster_means.loc[cluster, 'stock_to_sales_ratio']\n",
    "            mean_days_inv = cluster_means.loc[cluster, 'days_in_inventory']\n",
    "            if mean_sales < df['units_sold'].quantile(0.25) and mean_days_inv > df['days_in_inventory'].quantile(0.75):\n",
    "                labels[cluster] = 'Discontinue'\n",
    "            elif mean_velocity > df['sales_velocity'].quantile(0.75) and mean_sales > df['units_sold'].quantile(0.75):\n",
    "                labels[cluster] = 'Prioritize'\n",
    "            elif mean_stock_ratio > df['stock_to_sales_ratio'].quantile(0.75) and mean_velocity < df['sales_velocity'].quantile(0.25):\n",
    "                labels[cluster] = 'Reprice'\n",
    "            else:\n",
    "                labels[cluster] = 'Promote'\n",
    "        df_mapped['recommendation'] = df_mapped['cluster'].map(labels)\n",
    "        print(\"Cluster mapping completed.\")\n",
    "        return df_mapped\n",
    "    except Exception as e:\n",
    "        print(f\"Error in map_clusters_to_recommendations: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def kmeans_analysis_pipeline(df_preprocessed, n_clusters=4):\n",
    "    print(f\"Starting K-means analysis pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    try:\n",
    "        # Copy df_preprocessed\n",
    "        df_kmeans = df_preprocessed.copy()\n",
    "        \n",
    "        # Apply K-means clustering\n",
    "        df_kmeans, kmeans, silhouette = apply_kmeans_clustering(df_kmeans, n_clusters)\n",
    "        \n",
    "        # Analyze feature importance\n",
    "        importance, cluster_means = analyze_feature_importance(df_kmeans)\n",
    "        \n",
    "        # Map clusters to recommendations\n",
    "        df_kmeans = map_clusters_to_recommendations(df_kmeans, cluster_means)\n",
    "        \n",
    "        # Save results\n",
    "        df_kmeans.to_csv('kmeans_clustered_catalog_data.csv', index=False)\n",
    "        print(\"Clustered data saved to 'kmeans_clustered_catalog_data.csv'\")\n",
    "        print(\"\\nSample of clustered data:\")\n",
    "        print(df_kmeans[['product_id', 'units_sold', 'revenue', 'stock_level', 'cluster', 'recommendation']].head())\n",
    "        print(\"\\nCluster Distribution:\")\n",
    "        print(df_kmeans['recommendation'].value_counts())\n",
    "        \n",
    "        return df_kmeans, kmeans, importance, silhouette\n",
    "    except Exception as e:\n",
    "        print(f\"Error in kmeans_analysis_pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load df_preprocessed\n",
    "        df_preprocessed_original = pd.read_csv('preprocessed_catalog_data_original.csv')\n",
    "        \n",
    "        # Run K-means analysis\n",
    "        df_kmeans, kmeans, importance, silhouette = kmeans_analysis_pipeline(df_preprocessed_original)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9323004",
   "metadata": {},
   "source": [
    "## K-Means might not do well because it assumes the data are spread in a somewhat sperical manner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31c6ff",
   "metadata": {},
   "source": [
    "## We will use XGBoost (Reason given in the report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a28c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting XGBoost analysis pipeline at 2025-05-31 21:08:40\n",
      "Creating proxy labels...\n",
      "Proxy labels created successfully. Total products: 1000\n",
      "Training XGBoost classifier...\n",
      "Train set size: 800, Test set size: 200\n",
      "\n",
      "Train Accuracy: 0.991\n",
      "Test Accuracy: 0.975\n",
      "Cross-Validation Accuracy: 0.973 (+/- 0.027)\n",
      "Precision (weighted): 0.976\n",
      "Recall (weighted): 0.975\n",
      "F1-Score (weighted): 0.974\n",
      "\n",
      "Feature Importance (Top 10):\n",
      "                  Feature  Importance\n",
      "1              units_sold    0.331382\n",
      "10   stock_to_sales_ratio    0.256585\n",
      "11         sales_velocity    0.142392\n",
      "2                 revenue    0.110786\n",
      "12  days_since_last_sales    0.079993\n",
      "7       days_in_inventory    0.039610\n",
      "3                   price    0.022914\n",
      "4              cost_price    0.016339\n",
      "0             product_age    0.000000\n",
      "13        category_Beauty    0.000000\n",
      "Feature importance plot saved to 'feature_importance_xgb.png'\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "             Discontinue  Prioritize  Reprice  Promote\n",
      "Discontinue            5           0        1        0\n",
      "Prioritize             0          15        0        3\n",
      "Reprice                0           0       18        0\n",
      "Promote                0           0        1      157\n",
      "Confusion matrix plot saved to 'confusion_matrix_xgb.png'\n",
      "Classified data saved to 'xgb_classified_catalog_data.csv'\n",
      "\n",
      "Sample of classified data:\n",
      "  product_id  units_sold   revenue  stock_level recommendation  \\\n",
      "0      P0001   -0.335566 -0.615512    -0.855518        Promote   \n",
      "1      P0002   -0.861816 -0.670539    -0.466750        Reprice   \n",
      "2      P0003   -0.286764 -0.626559    -0.864719        Promote   \n",
      "3      P0004    0.749470  1.259712     0.149757     Prioritize   \n",
      "4      P0005   -0.298151  0.635662    -0.340228        Promote   \n",
      "\n",
      "  recommendation_pred  \n",
      "0             Promote  \n",
      "1             Reprice  \n",
      "2             Promote  \n",
      "3          Prioritize  \n",
      "4             Promote  \n",
      "\n",
      "Recommendation Distribution:\n",
      "recommendation_pred\n",
      "Promote        740\n",
      "Reprice        119\n",
      "Prioritize     110\n",
      "Discontinue     31\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import xgboost as xgb\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_proxy_labels(df):\n",
    "    print(\"Creating proxy labels...\")\n",
    "    try:\n",
    "        df_labeled = df.copy()\n",
    "        low_sales = df['units_sold'].quantile(0.25)\n",
    "        low_revenue = df['revenue'].quantile(0.25)\n",
    "        high_days_inv = df['days_in_inventory'].quantile(0.75)\n",
    "        high_days_since = df['days_since_last_sales'].quantile(0.75)\n",
    "        high_sales = df['units_sold'].quantile(0.75)\n",
    "        high_revenue = df['revenue'].quantile(0.75)\n",
    "        high_velocity = df['sales_velocity'].quantile(0.75)\n",
    "        high_stock_ratio = df['stock_to_sales_ratio'].quantile(0.75)\n",
    "        low_velocity = df['sales_velocity'].quantile(0.25)\n",
    "        conditions = [\n",
    "            (df_labeled['units_sold'] < low_sales) & \n",
    "            (df_labeled['revenue'] < low_revenue) & \n",
    "            (df_labeled['days_in_inventory'] > high_days_inv) & \n",
    "            (df_labeled['days_since_last_sales'] > high_days_since),\n",
    "            (df_labeled['units_sold'] > high_sales) & \n",
    "            (df_labeled['revenue'] > high_revenue) & \n",
    "            (df_labeled['sales_velocity'] > high_velocity),\n",
    "            (df_labeled['stock_to_sales_ratio'] > high_stock_ratio) & \n",
    "            (df_labeled['sales_velocity'] < low_velocity),\n",
    "        ]\n",
    "        choices = ['Discontinue', 'Prioritize', 'Reprice']\n",
    "        df_labeled['recommendation'] = np.select(conditions, choices, default='Promote')\n",
    "        \n",
    "        # Ensure all classes are represented\n",
    "        label_map = {'Discontinue': 0, 'Prioritize': 1, 'Reprice': 2, 'Promote': 3}\n",
    "        class_counts = df_labeled['recommendation'].value_counts()\n",
    "        min_samples = 1  # Minimum samples per class\n",
    "        for label in label_map.keys():\n",
    "            if label not in class_counts or class_counts[label] < min_samples:\n",
    "                # Assign the label to a random sample\n",
    "                idx = df_labeled.sample(1).index\n",
    "                df_labeled.loc[idx, 'recommendation'] = label\n",
    "        \n",
    "        print(f\"Proxy labels created successfully. Total products: {len(df_labeled)}\")\n",
    "        return df_labeled\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_proxy_labels: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def train_xgboost_classifier(df):\n",
    "    print(\"Training XGBoost classifier...\")\n",
    "    try:\n",
    "        # Verify required columns\n",
    "        required_cols = ['product_id', 'last_sale_date', 'recommendation']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        X = df.drop(columns=['product_id', 'last_sale_date', 'recommendation'])\n",
    "        y = df['recommendation']\n",
    "        \n",
    "        # Verify data integrity\n",
    "        if X.empty or y.empty:\n",
    "            raise ValueError(\"Feature matrix or target is empty\")\n",
    "        if X.isna().any().any() or y.isna().any():\n",
    "            raise ValueError(\"NaN values detected in features or target\")\n",
    "        if len(X) != 1000:\n",
    "            print(f\"Warning: Expected 1000 products, found {len(X)}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        label_map = {'Discontinue': 0, 'Prioritize': 1, 'Reprice': 2, 'Promote': 3}\n",
    "        y_encoded = y.map(label_map)\n",
    "        \n",
    "        # Verify label mapping\n",
    "        if y_encoded.isna().any():\n",
    "            raise ValueError(\"NaN values in encoded labels\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Verify test set size\n",
    "        print(f\"Train set size: {len(X_train)}, Test set size: {len(X_test)}\")\n",
    "        \n",
    "        # Train XGBoost with stronger regularization\n",
    "        xgb_clf = xgb.XGBClassifier(\n",
    "            n_estimators=50,            \n",
    "            max_depth=3,                \n",
    "            min_child_weight=10,        \n",
    "            learning_rate=0.05,       \n",
    "            reg_alpha=0.5,            \n",
    "            reg_lambda=2.0,         \n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss'\n",
    "        )\n",
    "        xgb_clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred_train = xgb_clf.predict(X_train)\n",
    "        y_pred_test = xgb_clf.predict(X_test)\n",
    "        \n",
    "        # Metrics\n",
    "        train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "        cv_scores = cross_val_score(xgb_clf, X, y_encoded, cv=5, scoring='accuracy')\n",
    "        precision = precision_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred_test, average='weighted', zero_division=0)\n",
    "        \n",
    "        print(f\"\\nTrain Accuracy: {train_accuracy:.3f}\")\n",
    "        print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "        print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        print(f\"Precision (weighted): {precision:.3f}\")\n",
    "        print(f\"Recall (weighted): {recall:.3f}\")\n",
    "        print(f\"F1-Score (weighted): {f1:.3f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        importance = pd.DataFrame({\n",
    "            'Feature': X.columns,\n",
    "            'Importance': xgb_clf.feature_importances_\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        print(\"\\nFeature Importance (Top 10):\")\n",
    "        print(importance.head(10))\n",
    "        \n",
    "        # Plot feature importance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance.head(10))\n",
    "        plt.title('Top 10 Feature Importance from XGBoost')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.ylabel('Feature')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance_xgb.png')\n",
    "        print(\"Feature importance plot saved to 'feature_importance_xgb.png'\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred_test)\n",
    "        print(\"\\nConfusion Matrix (Test Set):\")\n",
    "        cm_df = pd.DataFrame(cm, index=label_map.keys(), columns=label_map.keys())\n",
    "        print(cm_df)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=label_map.keys(), yticklabels=label_map.keys())\n",
    "        plt.title('Confusion Matrix for XGBoost Classifier')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('confusion_matrix_xgb.png')\n",
    "        print(\"Confusion matrix plot saved to 'confusion_matrix_xgb.png'\")\n",
    "        plt.close()\n",
    "        \n",
    "        # Add predictions to df\n",
    "        df['recommendation_pred'] = xgb_clf.predict(X)\n",
    "        df['recommendation_pred'] = df['recommendation_pred'].map({v: k for k, v in label_map.items()})\n",
    "        \n",
    "        return df, xgb_clf, importance, cm\n",
    "    except Exception as e:\n",
    "        print(f\"Error in train_xgboost_classifier: {e}\")\n",
    "        raise\n",
    "\n",
    "def xgb_analysis_pipeline(df_preprocessed):\n",
    "    print(f\"Starting XGBoost analysis pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    try:\n",
    "        # Verify df_preprocessed columns\n",
    "        expected_cols = ['product_id', 'last_sale_date', 'product_age', 'units_sold', 'revenue', \n",
    "                         'price', 'cost_price', 'stock_level', 'restock_frequency', \n",
    "                         'days_in_inventory', 'customer_rating', 'profit_margin', \n",
    "                         'stock_to_sales_ratio', 'sales_velocity', 'days_since_last_sales',\n",
    "                         'category_Electronics', 'category_Clothing', 'category_Home', \n",
    "                         'category_Beauty', 'category_Sports']\n",
    "        missing_cols = [col for col in expected_cols if col not in df_preprocessed.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns in df_preprocessed: {missing_cols}\")\n",
    "        \n",
    "        # Copy df_preprocessed\n",
    "        df_xgb = df_preprocessed.copy()\n",
    "        \n",
    "        # Apply proxy labels\n",
    "        df_xgb = create_proxy_labels(df_xgb)\n",
    "        \n",
    "        # Train XGBoost and evaluate\n",
    "        df_xgb, xgb_clf, importance, cm = train_xgboost_classifier(df_xgb)\n",
    "        \n",
    "        # Save results\n",
    "        df_xgb.to_csv('xgb_classified_catalog_data.csv', index=False)\n",
    "        print(\"Classified data saved to 'xgb_classified_catalog_data.csv'\")\n",
    "        print(\"\\nSample of classified data:\")\n",
    "        print(df_xgb[['product_id', 'units_sold', 'revenue', 'stock_level', 'recommendation', 'recommendation_pred']].head())\n",
    "        print(\"\\nRecommendation Distribution:\")\n",
    "        print(df_xgb['recommendation_pred'].value_counts())\n",
    "        \n",
    "        return df_xgb, xgb_clf, importance, cm\n",
    "    except Exception as e:\n",
    "        print(f\"Error in xgb_analysis_pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load df_preprocessed\n",
    "        df_preprocessed_original = pd.read_csv('preprocessed_catalog_data_original.csv')\n",
    "        \n",
    "        # Run XGBoost analysis\n",
    "        df_xgb, xgb_clf, importance, cm = xgb_analysis_pipeline(df_preprocessed_original)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b6f568f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_age</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>revenue</th>\n",
       "      <th>price</th>\n",
       "      <th>cost_price</th>\n",
       "      <th>stock_level</th>\n",
       "      <th>restock_frequency</th>\n",
       "      <th>days_in_inventory</th>\n",
       "      <th>last_sale_date</th>\n",
       "      <th>...</th>\n",
       "      <th>stock_to_sales_ratio</th>\n",
       "      <th>sales_velocity</th>\n",
       "      <th>days_since_last_sales</th>\n",
       "      <th>category_Beauty</th>\n",
       "      <th>category_Clothing</th>\n",
       "      <th>category_Electronics</th>\n",
       "      <th>category_Home</th>\n",
       "      <th>category_Sports</th>\n",
       "      <th>recommendation</th>\n",
       "      <th>recommendation_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P0001</td>\n",
       "      <td>-0.564022</td>\n",
       "      <td>-0.335566</td>\n",
       "      <td>-0.615512</td>\n",
       "      <td>-0.671077</td>\n",
       "      <td>-0.627312</td>\n",
       "      <td>-0.855518</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>1.296641</td>\n",
       "      <td>2024-12-23</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.751089</td>\n",
       "      <td>-0.682131</td>\n",
       "      <td>1.513440</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P0002</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.861816</td>\n",
       "      <td>-0.670539</td>\n",
       "      <td>1.359982</td>\n",
       "      <td>1.141727</td>\n",
       "      <td>-0.466750</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.588407</td>\n",
       "      <td>2025-03-04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973685</td>\n",
       "      <td>-0.785109</td>\n",
       "      <td>0.405253</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Reprice</td>\n",
       "      <td>Reprice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P0003</td>\n",
       "      <td>0.702826</td>\n",
       "      <td>-0.286764</td>\n",
       "      <td>-0.626559</td>\n",
       "      <td>-0.701489</td>\n",
       "      <td>-0.653256</td>\n",
       "      <td>-0.864719</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.285584</td>\n",
       "      <td>2025-04-14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.756498</td>\n",
       "      <td>-0.468591</td>\n",
       "      <td>-0.234686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P0004</td>\n",
       "      <td>0.100151</td>\n",
       "      <td>0.749470</td>\n",
       "      <td>1.259712</td>\n",
       "      <td>-0.009859</td>\n",
       "      <td>0.167389</td>\n",
       "      <td>0.149757</td>\n",
       "      <td>-0.081042</td>\n",
       "      <td>-0.647236</td>\n",
       "      <td>2025-05-11</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.623355</td>\n",
       "      <td>0.579670</td>\n",
       "      <td>-0.656109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Prioritize</td>\n",
       "      <td>Prioritize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P0005</td>\n",
       "      <td>-1.015004</td>\n",
       "      <td>-0.298151</td>\n",
       "      <td>0.635662</td>\n",
       "      <td>0.790042</td>\n",
       "      <td>0.569737</td>\n",
       "      <td>-0.340228</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.526685</td>\n",
       "      <td>2025-05-09</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.554932</td>\n",
       "      <td>-0.370410</td>\n",
       "      <td>-0.624893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>P0996</td>\n",
       "      <td>-1.121599</td>\n",
       "      <td>-0.891098</td>\n",
       "      <td>-0.829655</td>\n",
       "      <td>-0.738040</td>\n",
       "      <td>-0.617909</td>\n",
       "      <td>-0.584071</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>1.070609</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>...</td>\n",
       "      <td>1.683554</td>\n",
       "      <td>-0.794734</td>\n",
       "      <td>1.888038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Discontinue</td>\n",
       "      <td>Discontinue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>P0997</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.773972</td>\n",
       "      <td>-0.749871</td>\n",
       "      <td>-0.488645</td>\n",
       "      <td>-0.598562</td>\n",
       "      <td>-0.570268</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.181549</td>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219523</td>\n",
       "      <td>-0.749393</td>\n",
       "      <td>-0.859017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>P0998</td>\n",
       "      <td>-1.047802</td>\n",
       "      <td>-0.768279</td>\n",
       "      <td>-0.734273</td>\n",
       "      <td>-0.402125</td>\n",
       "      <td>-0.461608</td>\n",
       "      <td>-0.563367</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>-0.571892</td>\n",
       "      <td>2025-04-27</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.229274</td>\n",
       "      <td>-0.691149</td>\n",
       "      <td>-0.437593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Promote</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>P0999</td>\n",
       "      <td>1.727784</td>\n",
       "      <td>-0.705649</td>\n",
       "      <td>-0.698460</td>\n",
       "      <td>-0.446483</td>\n",
       "      <td>-0.440341</td>\n",
       "      <td>-0.452948</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>1.598017</td>\n",
       "      <td>2024-11-29</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264937</td>\n",
       "      <td>-0.761935</td>\n",
       "      <td>1.888038</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Promote</td>\n",
       "      <td>Promote</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>P1000</td>\n",
       "      <td>-0.383629</td>\n",
       "      <td>-0.789426</td>\n",
       "      <td>-0.826697</td>\n",
       "      <td>-0.886766</td>\n",
       "      <td>-0.792475</td>\n",
       "      <td>0.131354</td>\n",
       "      <td>-0.690378</td>\n",
       "      <td>0.060999</td>\n",
       "      <td>2025-05-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099018</td>\n",
       "      <td>-0.750872</td>\n",
       "      <td>-0.734150</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Reprice</td>\n",
       "      <td>Reprice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_id  product_age  units_sold   revenue     price  cost_price  \\\n",
       "0        P0001    -0.564022   -0.335566 -0.615512 -0.671077   -0.627312   \n",
       "1        P0002     1.727784   -0.861816 -0.670539  1.359982    1.141727   \n",
       "2        P0003     0.702826   -0.286764 -0.626559 -0.701489   -0.653256   \n",
       "3        P0004     0.100151    0.749470  1.259712 -0.009859    0.167389   \n",
       "4        P0005    -1.015004   -0.298151  0.635662  0.790042    0.569737   \n",
       "..         ...          ...         ...       ...       ...         ...   \n",
       "995      P0996    -1.121599   -0.891098 -0.829655 -0.738040   -0.617909   \n",
       "996      P0997     1.727784   -0.773972 -0.749871 -0.488645   -0.598562   \n",
       "997      P0998    -1.047802   -0.768279 -0.734273 -0.402125   -0.461608   \n",
       "998      P0999     1.727784   -0.705649 -0.698460 -0.446483   -0.440341   \n",
       "999      P1000    -0.383629   -0.789426 -0.826697 -0.886766   -0.792475   \n",
       "\n",
       "     stock_level  restock_frequency  days_in_inventory last_sale_date  ...  \\\n",
       "0      -0.855518          -0.081042           1.296641     2024-12-23  ...   \n",
       "1      -0.466750          -0.690378           0.588407     2025-03-04  ...   \n",
       "2      -0.864719          -0.690378          -0.285584     2025-04-14  ...   \n",
       "3       0.149757          -0.081042          -0.647236     2025-05-11  ...   \n",
       "4      -0.340228          -0.690378          -0.526685     2025-05-09  ...   \n",
       "..           ...                ...                ...            ...  ...   \n",
       "995    -0.584071          -0.690378           1.070609     2024-11-29  ...   \n",
       "996    -0.570268          -0.690378           0.181549     2025-05-24  ...   \n",
       "997    -0.563367          -0.690378          -0.571892     2025-04-27  ...   \n",
       "998    -0.452948          -0.690378           1.598017     2024-11-29  ...   \n",
       "999     0.131354          -0.690378           0.060999     2025-05-16  ...   \n",
       "\n",
       "     stock_to_sales_ratio  sales_velocity  days_since_last_sales  \\\n",
       "0               -0.751089       -0.682131               1.513440   \n",
       "1                0.973685       -0.785109               0.405253   \n",
       "2               -0.756498       -0.468591              -0.234686   \n",
       "3               -0.623355        0.579670              -0.656109   \n",
       "4               -0.554932       -0.370410              -0.624893   \n",
       "..                    ...             ...                    ...   \n",
       "995              1.683554       -0.794734               1.888038   \n",
       "996             -0.219523       -0.749393              -0.859017   \n",
       "997             -0.229274       -0.691149              -0.437593   \n",
       "998             -0.264937       -0.761935               1.888038   \n",
       "999              1.099018       -0.750872              -0.734150   \n",
       "\n",
       "     category_Beauty  category_Clothing  category_Electronics  category_Home  \\\n",
       "0                0.0                0.0                   0.0            1.0   \n",
       "1                0.0                0.0                   1.0            0.0   \n",
       "2                0.0                1.0                   0.0            0.0   \n",
       "3                0.0                0.0                   1.0            0.0   \n",
       "4                0.0                0.0                   0.0            1.0   \n",
       "..               ...                ...                   ...            ...   \n",
       "995              0.0                0.0                   1.0            0.0   \n",
       "996              0.0                1.0                   0.0            0.0   \n",
       "997              0.0                1.0                   0.0            0.0   \n",
       "998              0.0                0.0                   0.0            0.0   \n",
       "999              1.0                0.0                   0.0            0.0   \n",
       "\n",
       "     category_Sports  recommendation  recommendation_pred  \n",
       "0                0.0         Promote              Promote  \n",
       "1                0.0         Reprice              Reprice  \n",
       "2                0.0         Promote              Promote  \n",
       "3                0.0      Prioritize           Prioritize  \n",
       "4                0.0         Promote              Promote  \n",
       "..               ...             ...                  ...  \n",
       "995              0.0     Discontinue          Discontinue  \n",
       "996              0.0         Promote              Promote  \n",
       "997              0.0         Promote              Promote  \n",
       "998              1.0         Promote              Promote  \n",
       "999              0.0         Reprice              Reprice  \n",
       "\n",
       "[1000 rows x 22 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a63e1d0",
   "metadata": {},
   "source": [
    "## Now we will work on Upgradability and Tuning our model (We will work on XGBoost since it is very efficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8303790b",
   "metadata": {},
   "source": [
    "## Fine Tuning using Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53637864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting tuned XGBoost pipeline at 2025-05-31 21:09:12\n",
      "Starting XGBoost analysis pipeline at 2025-05-31 21:09:12\n",
      "Creating proxy labels...\n",
      "Proxy labels created successfully. Total products: 1000\n",
      "Training XGBoost classifier...\n",
      "Train set size: 800, Test set size: 200\n",
      "\n",
      "Train Accuracy: 0.991\n",
      "Test Accuracy: 0.975\n",
      "Cross-Validation Accuracy: 0.973 (+/- 0.027)\n",
      "Precision (weighted): 0.976\n",
      "Recall (weighted): 0.975\n",
      "F1-Score (weighted): 0.974\n",
      "\n",
      "Feature Importance (Top 10):\n",
      "                  Feature  Importance\n",
      "1              units_sold    0.331382\n",
      "10   stock_to_sales_ratio    0.256585\n",
      "11         sales_velocity    0.142392\n",
      "2                 revenue    0.110786\n",
      "12  days_since_last_sales    0.079993\n",
      "7       days_in_inventory    0.039610\n",
      "3                   price    0.022914\n",
      "4              cost_price    0.016339\n",
      "0             product_age    0.000000\n",
      "13        category_Beauty    0.000000\n",
      "Feature importance plot saved to 'feature_importance_xgb.png'\n",
      "\n",
      "Confusion Matrix (Test Set):\n",
      "             Discontinue  Prioritize  Reprice  Promote\n",
      "Discontinue            5           0        1        0\n",
      "Prioritize             0          15        0        3\n",
      "Reprice                0           0       18        0\n",
      "Promote                0           0        1      157\n",
      "Confusion matrix plot saved to 'confusion_matrix_xgb.png'\n",
      "Classified data saved to 'xgb_classified_catalog_data.csv'\n",
      "\n",
      "Sample of classified data:\n",
      "  product_id  units_sold   revenue  stock_level recommendation  \\\n",
      "0      P0001   -0.335566 -0.615512    -0.855518        Promote   \n",
      "1      P0002   -0.861816 -0.670539    -0.466750        Reprice   \n",
      "2      P0003   -0.286764 -0.626559    -0.864719        Promote   \n",
      "3      P0004    0.749470  1.259712     0.149757     Prioritize   \n",
      "4      P0005   -0.298151  0.635662    -0.340228        Promote   \n",
      "\n",
      "  recommendation_pred  \n",
      "0             Promote  \n",
      "1             Reprice  \n",
      "2             Promote  \n",
      "3          Prioritize  \n",
      "4             Promote  \n",
      "\n",
      "Recommendation Distribution:\n",
      "recommendation_pred\n",
      "Promote        740\n",
      "Reprice        119\n",
      "Prioritize     110\n",
      "Discontinue     31\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== XGBoost Training Complete ===\n",
      "Starting threshold tuning at 2025-05-31 21:09:14\n",
      "\n",
      "=== Tuning Results ===\n",
      "Tuned Thresholds: {'Discontinue': 0.456666202282873, 'Prioritize': 0.17997993265440235, 'Reprice': 0.4673991135726938, 'Promote': 0.36696688891121754}\n",
      "Precision (weighted): 0.990\n",
      "Recall (weighted): 0.990\n",
      "F1-Score (weighted): 0.990\n",
      "\n",
      "Tuned Recommendation Distribution:\n",
      "recommendation_tuned\n",
      "Promote        740\n",
      "Reprice        117\n",
      "Prioritize     110\n",
      "Discontinue     33\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of Predictions (Original vs Tuned):\n",
      "  product_id recommendation recommendation_pred recommendation_tuned\n",
      "0      P0001        Promote             Promote              Promote\n",
      "1      P0002        Reprice             Reprice              Reprice\n",
      "2      P0003        Promote             Promote              Promote\n",
      "3      P0004     Prioritize          Prioritize           Prioritize\n",
      "4      P0005        Promote             Promote              Promote\n",
      "Tuned data saved to 'xgb_tuned_catalog_data.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datetime import datetime\n",
    "import xgboost as xgb\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real\n",
    "\n",
    "def tune_thresholds(df, xgb_clf, n_calls=50):\n",
    "    print(f\"Starting threshold tuning at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    try:\n",
    "        # Features and true labels\n",
    "        X = df.drop(columns=['product_id', 'last_sale_date', 'recommendation', 'recommendation_pred'])\n",
    "        y = df['recommendation']\n",
    "        label_map = {'Discontinue': 0, 'Prioritize': 1, 'Reprice': 2, 'Promote': 3}\n",
    "        y_encoded = y.map(label_map)\n",
    "        \n",
    "        # Get probability predictions\n",
    "        y_prob = xgb_clf.predict_proba(X)\n",
    "        \n",
    "        # Define custom scoring: increased weight for Discontinue for caution\n",
    "        def custom_score(y_true, y_pred, weights={'Discontinue': 0.5, 'Promote': 0.3, 'Prioritize': 0.15, 'Reprice': 0.05}):\n",
    "            precision = precision_score(y_true, y_pred, average=None, labels=list(label_map.values()), zero_division=0)\n",
    "            recall = recall_score(y_true, y_pred, average=None, labels=list(label_map.values()), zero_division=0)\n",
    "            score = 0\n",
    "            for i, label in enumerate(label_map.keys()):\n",
    "                score += weights[label] * (0.6 * precision[i] + 0.4 * recall[i])\n",
    "            return score\n",
    "        \n",
    "        # Objective function for Bayesian optimization (minimize negative score)\n",
    "        def objective(thresholds):\n",
    "            y_pred = np.argmax(np.where(y_prob >= thresholds, y_prob, 0), axis=1)\n",
    "            return -custom_score(y_encoded, y_pred)  # Negative for minimization\n",
    "        \n",
    "        # Define search space: thresholds for all classes (0.1 to 0.9)\n",
    "        space = [Real(0.1, 0.9, name=f'thresh_{label}') for label in label_map.keys()]\n",
    "        \n",
    "        # Run Bayesian optimization with more iterations\n",
    "        result = gp_minimize(\n",
    "            func=objective,\n",
    "            dimensions=space,\n",
    "            n_calls=50,  # Increased for more precision\n",
    "            n_random_starts=10,  # Initial random points\n",
    "            random_state=42,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Extract best thresholds\n",
    "        best_thresholds = {label: result.x[i] for i, label in enumerate(label_map.keys())}\n",
    "        \n",
    "        # Apply tuned thresholds\n",
    "        y_prob_tuned = np.where(y_prob >= list(best_thresholds.values()), y_prob, 0)\n",
    "        df['recommendation_tuned'] = np.argmax(y_prob_tuned, axis=1)\n",
    "        df['recommendation_tuned'] = df['recommendation_tuned'].map({v: k for k, v in label_map.items()})\n",
    "        \n",
    "        # Evaluate\n",
    "        precision = precision_score(y_encoded, df['recommendation_tuned'].map(label_map), \n",
    "                                   average='weighted', zero_division=0)\n",
    "        recall = recall_score(y_encoded, df['recommendation_tuned'].map(label_map), \n",
    "                              average='weighted', zero_division=0)\n",
    "        f1 = f1_score(y_encoded, df['recommendation_tuned'].map(label_map), \n",
    "                      average='weighted', zero_division=0)\n",
    "        print(f\"\\n=== Tuning Results ===\")\n",
    "        print(f\"Tuned Thresholds: {best_thresholds}\")\n",
    "        print(f\"Precision (weighted): {precision:.3f}\")\n",
    "        print(f\"Recall (weighted): {recall:.3f}\")\n",
    "        print(f\"F1-Score (weighted): {f1:.3f}\")\n",
    "        print(\"\\nTuned Recommendation Distribution:\")\n",
    "        print(df['recommendation_tuned'].value_counts())\n",
    "        print(\"\\nSample of Predictions (Original vs Tuned):\")\n",
    "        print(df[['product_id', 'recommendation', 'recommendation_pred', 'recommendation_tuned']].head())\n",
    "        \n",
    "        # Save results\n",
    "        df.to_csv('xgb_tuned_catalog_data.csv', index=False)\n",
    "        print(\"Tuned data saved to 'xgb_tuned_catalog_data.csv'\")\n",
    "        \n",
    "        return df, best_thresholds\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tune_thresholds: {e}\")\n",
    "        raise\n",
    "\n",
    "def tuned_xgb_pipeline(df_preprocessed):\n",
    "    print(f\"Starting tuned XGBoost pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    try:\n",
    "        # Apply proxy labels and train XGBoost\n",
    "        df_xgb, xgb_clf, importance, cm = xgb_analysis_pipeline(df_preprocessed)\n",
    "        print(\"\\n=== XGBoost Training Complete ===\")\n",
    "        \n",
    "        # Tune thresholds\n",
    "        df_tuned, best_thresholds = tune_thresholds(df_xgb, xgb_clf)\n",
    "        \n",
    "        return df_tuned, xgb_clf, best_thresholds\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tuned_xgb_pipeline: {e}\")\n",
    "        raise\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Load preprocessed data\n",
    "        df_preprocessed_original = pd.read_csv('preprocessed_catalog_data_original.csv')\n",
    "        \n",
    "        # Run tuned pipeline\n",
    "        df_tuned, xgb_clf, best_thresholds = tuned_xgb_pipeline(df_preprocessed_original)\n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
